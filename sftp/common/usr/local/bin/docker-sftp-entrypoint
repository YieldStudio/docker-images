#!/bin/bash
#set -Eeo pipefail

# shellcheck disable=2154
trap 's=$?; echo "$0: Error on line "$LINENO": $BASH_COMMAND"; exit $s' ERR

reArgsMaybe="^[^:[:space:]]+:.*$" # Smallest indication of attempt to use argument
reArgSkip='^([[:blank:]]*#.*|[[:blank:]]*)$' # comment or empty line

# Paths
userConfPath="/etc/sftp/users.conf"
userConfPathLegacy="/etc/sftp-users.conf"
userConfFinalPath="/var/run/sftp/users.conf"

function log() {
    echo "[$0] $*" >&2
}

# Allow running other programs, e.g. bash
if [[ -z "$1" || "$1" =~ $reArgsMaybe ]]; then
    startSshd=true
else
    startSshd=false
fi

function setupServer() {
  # Backward compatibility with legacy config path
  if [ ! -f "$userConfPath" ] && [ -f "$userConfPathLegacy" ]; then
      mkdir -p "$(dirname $userConfPath)"
      ln -s "$userConfPathLegacy" "$userConfPath"
  fi

  # Create users only on first run
  if [ ! -f "$userConfFinalPath" ]; then
      mkdir -p "$(dirname $userConfFinalPath)"

      if [ -f "$userConfPath" ]; then
          # Append mounted config to final config
          grep -v -E "$reArgSkip" < "$userConfPath" > "$userConfFinalPath"
      fi

      if $startSshd; then
          # Append users from arguments to final config
          for user in "$@"; do
              echo "$user" >> "$userConfFinalPath"
          done
      fi

      if [ -n "$SFTP_USERS" ]; then
          # Append users from environment variable to final config
          IFS=" " read -r -a usersFromEnv <<< "$SFTP_USERS"
          for user in "${usersFromEnv[@]}"; do
              echo "$user" >> "$userConfFinalPath"
          done
      fi

      # Check that we have users in config
      if [ -f "$userConfFinalPath" ] && [ "$(wc -l < "$userConfFinalPath")" -gt 0 ]; then
          # Import users from final conf file
          while IFS= read -r user || [[ -n "$user" ]]; do
              create-sftp-user "$user"
          done < "$userConfFinalPath"
      elif $startSshd; then
          log "FATAL: No users provided!"
          exit 3
      fi

      if [ -n "$SSH_ED25519_KEY" ]; then
          base64 -d <<< "$SSH_ED25519_KEY" > /etc/ssh/ssh_host_ed25519_key
      fi

      if [ -n "$SSH_RSA_KEY" ]; then
          base64 -d <<< "$SSH_RSA_KEY" > /etc/ssh/ssh_host_rsa_key
      fi

      # Generate unique ssh keys for this container, if needed
      if [ ! -f /etc/ssh/ssh_host_ed25519_key ]; then
          ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -N ''
      fi
      if [ ! -f /etc/ssh/ssh_host_rsa_key ]; then
          ssh-keygen -t rsa -b 4096 -f /etc/ssh/ssh_host_rsa_key -N ''
      fi

      # Restrict access from other users
      chmod 600 /etc/ssh/ssh_host_ed25519_key || true
      chmod 600 /etc/ssh/ssh_host_rsa_key || true
  else
      log "Users already created. Skipping."
  fi

  # Source custom scripts, if any
  if [ -d /etc/sftp.d ]; then
      for f in /etc/sftp.d/*; do
          if [ -x "$f" ]; then
              log "Running $f ..."
              $f
          else
              log "Could not run $f, because it's missing execute permission (+x)."
          fi
      done
      unset f
  fi
}

function setupS3FS() {
  S3FS_DEBUG=${S3FS_DEBUG:-"0"}

  # Root directory for settings and bucket.
  AWS_S3_ROOTDIR=${AWS_S3_ROOTDIR:-"/opt/s3fs"}

  # Where are we going to mount the remote bucket resource in our container.
  AWS_S3_MOUNT=${AWS_S3_MOUNT:-"${AWS_S3_ROOTDIR%/}/bucket"}

  # Create destination directory if it does not exist.
  if [ ! -d "$AWS_S3_MOUNT" ]; then
      mkdir -p "$AWS_S3_MOUNT"
  fi

  AWS_S3_AUTHFILE=${AWS_S3_AUTHFILE:-"${AWS_S3_ROOTDIR%/}/passwd-s3fs"}

  if [ -z "${AWS_S3_CREDENTIALS}" -a -n "${AWS_S3_ACCESS_KEY_ID}" -a -n "${AWS_S3_SECRET_ACCESS_KEY}" ]; then
      AWS_S3_CREDENTIALS="${AWS_S3_ACCESS_KEY_ID}:${AWS_S3_SECRET_ACCESS_KEY}"
  fi

  # Create or use authorisation file
  if [ -n "${AWS_S3_CREDENTIALS}" ]; then
      echo "${AWS_S3_CREDENTIALS}" > $AWS_S3_AUTHFILE
      chmod 400 $AWS_S3_AUTHFILE
  else
      echo "Error: You need to provide some AWS credentials"
      exit 128
  fi

  if [ -z "${AWS_S3_BUCKET}" ]; then
      echo "Error: AWS_S3_BUCKET not provided"
      exit 128
  fi

  if [ -z "${AWS_S3_URL}" ]; then
      AWS_S3_URL="https://s3.amazonaws.com"
  fi

  if [ -z "${AWS_S3_REGION}" ]; then
      AWS_S3_REGION="us-east-1"
  fi

  if [ -n "${AWS_S3_SECRET_ACCESS_KEY}" ]; then
      unset AWS_S3_SECRET_ACCESS_KEY
  fi

  # Debug options
  DEBUG_OPTS=
  if [ $S3FS_DEBUG = "true" ] || [ $S3FS_DEBUG = "1" ]; then
      DEBUG_OPTS="-d"
  fi

  # Additional mount options
  if [ -n "${S3FS_ARGS}" ]; then
      S3FS_ARGS="-o $S3FS_ARGS"
  fi

  # Mount and verify that something is present. davfs2 always creates a lost+found
  # sub-directory, so we can use the presence of some file/dir as a marker to
  # detect that mounting was a success. Execute the command on success.
  s3fs ${DEBUG_OPTS} ${S3FS_ARGS} \
      -o passwd_file=${AWS_S3_AUTHFILE} \
      -o url=${AWS_S3_URL} \
      -o endpoint=${AWS_S3_REGION} \
      ${AWS_S3_BUCKET} ${AWS_S3_MOUNT}

  # s3fs can claim to have a mount even though it didn't succeed.
  # Doing an operation actually forces it to detect that and remove the mount.
  echo "Verifying mount by listing ${AWS_S3_MOUNT}:"
  ls "${AWS_S3_MOUNT}"

  mounted=$(mount | grep fuse.s3fs | grep "${AWS_S3_MOUNT}")
  if [ -n "${mounted}" ]; then
      echo "Mounted bucket ${AWS_S3_BUCKET} onto ${AWS_S3_MOUNT}"
  else
      echo "Mount failure"
      exit 128
  fi
}

# Setup S3FS if needed
if [ "$ENABLE_S3FS" = "true" ] || [ "$ENABLE_S3FS" = "1" ]; then
    setupS3FS
fi

# Setup server and users
setupServer

# Start sshd or other command
if $startSshd; then
    log "Executing sshd"
    exec /usr/sbin/sshd -D -e
else
    log "Executing $*"
    exec "$@"
fi
